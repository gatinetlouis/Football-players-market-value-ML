---
title: "Introduction to Machine Learning - FIFA 19 Case"
author: "A. Piguet - L. Gatinet - J. Guillot"
output: html_document
---

# Introduction:

[FIFA 19](https://en.wikipedia.org/wiki/FIFA_19) is a football simulation video game developed by EA Vancouver as part of Electronic Arts' FIFA series. It is the 26^th^ installment in the FIFA series, and was released on 28 September 2018 for PlayStation 3, PlayStation 4, Xbox 360, Xbox One, Nintendo Switch, and Microsoft Windows. 

With more than 45 million unique players worldwide, FIFA is known as the best football video game in the world.

FIFA 19 aggregates each year data on football profesionnal players and their transfers. During the whole case, we will try to find the best algorithm to predict the Transfer Value of players through historical data presented in FIFA 19.

This dataset has been found on [Karan Gadiya's Kaggle page](https://www.kaggle.com/karangadiya/fifa19).

```{r setup, include=FALSE}
knitr::opts_chunk$set( warning = FALSE,
                       echo = TRUE,
                       message = FALSE,
                       highlight = TRUE)
```

```{r}
my_dir = getwd()
setwd(my_dir)
```

```{r}
library(readr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggExtra)
library(glmnet)
library(ranger)
library(caret)
library(ggcorrplot)
library(visNetwork)
library(qdapRegex)
library(KernSmooth)
library(class)
library(rpart)
```

```{r}
data = read_csv("fifadata.csv")
```

***

# I-Data cleaning using dplyr:

#### Removing useless, noisy variables
For various reason, we need to remove some variables from our dataset in order to apply machine learning algorithms.

**X1** is an "ID" column we do not need.
**Photo**, **Flag** and **Club Logo** are just url links leading to logos.
**Special** does not relate to anything documented on the dataset source.
For the other removed variables, we considered they did not add anything when explaining a player's financial value at a given moment.

```{r}
data = data %>% select(-X1,
                       -Photo, 
                       -Flag,
                       -Special, 
                       -`Body Type`,
                       -`Real Face`, 
                       -`Jersey Number`,
                       -Joined, 
                       -`Loaned From`,
                       -(LS:RB), 
                       )
```


#### Converting numeric values

We first want some values to be converted into International Metric System or remove units so that they are considered as double or integer by R.
```{r}
# Financial variables
data$Value = ifelse(substring(data$Value, 
                              nchar(data$Value)) == "M",
                              as.numeric(substring(data$Value,2,nchar(data$Value)-1)),
                              as.numeric(substring(data$Value,2,nchar(data$Value)-1)) / 1000)

data$Wage = as.numeric(substring(data$Wage,2,nchar(data$Wage)-1))

data$`Release Clause` = as.numeric(substring(data$`Release Clause`,2,nchar(data$`Release Clause`)-1))


# Players' measurements
data$Weight = as.numeric(substring(data$Weight, 1, 3)) * 0.45359237

data = data %>% 
  separate(Height, c('feet', 'inches'), "'", convert = TRUE) %>%
  mutate(Height = (12*feet + inches)*2.54) %>% 
  select(-inches, -feet)


# Renaming our variables
data = data %>% rename(`Value (in M€)` = Value,
                        `Wage (in K€)` = Wage,
                        `Release Clause (in M€)` = `Release Clause`,
                        `Weight (in kg)` = Weight,
                        `Height (in cm)` = Height)
```


#### Let's adjust some variables


##### The "Championship" variable
Here we use a csv file ('club.csv') that matches each club with the corresponding championship to create a **"Championship" variable**. To further use the 'Championship' variable in a relevant manner, we decided to **remove championships for which some clubs are missing** (namely championships FIFA 19 did not get the right to refer to).
Details can be found [on the FIFA 19 Wikipedia page](https://fr.wikipedia.org/wiki/FIFA_19#Championnats)

Looking at the ranking of our leagues, we remark a gap in average player tranfer value between the 6th championship (the Portuguese one) and those following. For visualization purposes and as these first 6 championships are the most known ones, we decide to keep them and aggregate the others under an "Other" category.

```{r}
num_club = as.numeric(ex_between(data$`Club Logo`, "light/", ".png"))
data$num_club = num_club

`%nin%` = Negate(`%in%`) # creating a 'not in' operator for convenience
club_table = read_csv('club.csv')
clean_club_table = club_table %>% filter(league_name %nin% c("South African Premier Division",
                                                        "Finnish Veikkausliiga",
                                                        "Greek Super League",
                                                        "Russian Premier League",
                                                        "Croatian Prva HNL",
                                                        "Czech Republic Gambrinus Liga",
                                                        "Ukrainian Premier League"))

group_club = clean_club_table %>% group_by(url) %>% summarize(league_name=first(league_name))
data = left_join(data,group_club,by=c('num_club'='url'))

df_league = data %>% 
  group_by(league_name) %>% 
  summarise(value=mean(`Value (in M€)`)) %>% 
  arrange(desc(value))


# Aggregating minor championships into an "Others" category
df_league_others = df_league %>% slice(7:nrow(df_league))
data[data$league_name %in% df_league_others$league_name, "league_name"] = "Others"

# Finally, we remove the "Club Logo" variable since we do not need it anymore
data = data %>% select(-`Club Logo`)

```


##### The "Field Position" variable

We decided to use the information included in the **Position** variable to create a more meaningful variable telling what line the player plays on on the field (Goalkeeper, Defenser, Mitfielder, Attack)
```{r}
data$Field_Position = rep(0,nrow(data))
for (i in 1:nrow(data)) {
  if (data$Position[i] %in% c("LW", "LF", "RW", "RF", 'CF', 'LS', 'RS', 'ST')) data$Field_Position[i] = "Attack"
  if (data$Position[i] %in% c('CDM', 'LDM', 'RDM', 'CM', 'LCM', 'RCM', 'CAM', 'LAM', 'RAM', 'LM', 'RM')) data$Field_Position[i] = "Mitfielder"
  if (data$Position[i] %in% c('CB', 'LCB', 'RCB', 'LB', 'RB', 'LWB', 'RWB')) data$Field_Position[i] = "Defenser"
  if (data$Position[i] %in% c("GK")) data$Field_Position[i] = "Goalkeeper"
}
```


##### Splitting the "Work Rate" column into its 2 components (Offensive / Defensive)

```{r}
data = data %>% separate(`Work Rate`, c('Offensive Work Rate', 'Defensive Work Rate'), "/ ")

data$`Offensive Work Rate` = factor(data$`Offensive Work Rate`, 
                                    levels=c("Low", "Medium", 'High'))
data$`Defensive Work Rate` = factor(data$`Defensive Work Rate`, 
                                    levels=c("Low", "Medium", 'High'))
data$Field_Position = factor(data$Field_Position, 
                             levels=c("Goalkeeper", "Defenser", 'Mitfielder', "Attack"))
data$`Preferred Foot` = factor(data$`Preferred Foot`, 
                               levels=c("Left", "Right"))
data$`International Reputation` = factor(data$`International Reputation`, 
                                         levels=1:5)
data$`Weak Foot` = factor(data$`Weak Foot`, 
                          levels=1:5)
data$`Skill Moves` = factor(data$`Skill Moves`, 
                            levels=1:5)
```


##### The "Contract Duration" column

We create a column that captures the remaining duration of the player's contract. And then, we substract 2018 from the contract end date as the year 2018 is the reference year for the data in FIFA 19 (data when the game is release is as of August 31st 2018, for use during season 2018-2019).

```{r}
text = NULL; first = NULL; last = NULL
for (i in 1:nrow(data)) {
  text[i] = data$`Contract Valid Until`[i] 
  first[i] = nchar(text[i])-4
  last[i] = nchar(text[i])
  data$`Contract Valid Until`[i] = substring(text[i],first[i],last[i])
}

data$`Remaining Contract Duration` = as.numeric(data$`Contract Valid Until`) - 2018
data <- data %>% select(-`Contract Valid Until`, -Position)
```


#### Looking for NA values

```{r}
na_df = data[which(apply(is.na(data),1,any)),]
head(na_df)
```

All remaining NA's are **players who have been loaned to another team (1264 players)** and **players for whom FIFA did not gather the information or was denied the access (300).**
Considering that we have more than 15,000 rows and that players' for whom we have NA's do not follow a particular scheme (we are not removing any precious information), we decided to omit these observations.


```{r}
rm(na_df)
data = na.omit(data)
```

***

# II-Data Exploration using dplyr & ggplot


#### Distribution of the Players' value

Let's begin this exploratory phase with looking at the ***distribution of the players' value***. The football players' transfer market has been very speculative and insanely growing these past few years. As a result the top 100 players almost do not stand in the same transfer market as other average less gifted players.
Considering this issue and for visualization purposes we will sometimes need to use a log scale for the players' value.

```{r}
med = median(data$`Value (in M€)`)

ggplot(data) + 
  aes(x=`Value (in M€)`) + 
  geom_histogram() + 
  geom_vline(aes(xintercept=med),col="red") +
  scale_x_log10() +
  theme_bw()
```

Looking at this histogram, we see that the distribution is skewed to the right, due to extreme positive outliers. The median stands at 0.675 meaning that **more than 50% of the players are valued at less than 5 million euros**.


#### Distribution of the Players' value regarding the league

```{r}
df_medians = data %>% group_by(league_name) %>% summarise(median=quantile(log(`Value (in M€)`),c(0.5)))

ggplot(data) + 
  aes(log(`Value (in M€)`)) + 
  geom_density(aes(fill = league_name, col = league_name),alpha=0.3) +
  facet_grid(league_name~.) +
  theme_bw() +
  theme(axis.title=element_text(face="bold"))+
  scale_color_discrete("League Names")+
  scale_fill_discrete("League Names")+
  scale_x_continuous('log of Value (in M€)')+
  geom_vline(data=df_medians,aes(xintercept=median, col = league_name),alpha = 0.9)+
  theme(strip.text = element_blank())

```

Looking at the density distribution of each league we have more insights : 
- The German Bundesliga and the Italian Serie A have quite similar distributions skewed to the left
- The English Premier League and the Spain Primera Division both have the highest median but the first one has a smoother distribution
- The French Ligue 1 and the Portuguese Liga almost have the same median but the second one is more skewed to the right.


#### Plot of the Value vs Wage

After looking at the distribution of values, several questions arise. We can for instance look at the way the Players' Value and their Wage are correlated. Among all players, Attackers are those known to be the most expensive (some rewards, such as *Le Ballon d'Or*, testify for the fact that Attackers are attached higher values, sometimes even independently from what they bring to their team). We decided to look at this correlation depending on the Field Position.

```{r}
ggplot(data) +
  aes(x=`Wage (in K€)`, y=`Value (in M€)`, colour=Field_Position) +
  geom_point(stroke=F) +
  geom_smooth(method='lm') +
  scale_x_continuous(limits=c(0,300)) +
  scale_color_discrete("Field Position") +
  theme_bw() +
  theme(axis.title=element_text(face="bold"))
```

As expected, the players' market value is positively correlated to the players' wages. Looking at the different field positions we see that, for a given market value, **Defensers get a better wage than Attackers**. This reflects the law of offer and supply: **there are more Attackers valued at 30 M€ than Defensers so the latter get better paid**.


#### Plot of the Value vs Work Rate

```{r}
ggplot(data) +
  aes(x=`Offensive Work Rate`, y=`Value (in M€)`, fill=`Offensive Work Rate`) +
  geom_boxplot(show.legend = F) +
  scale_y_log10() +
  theme_bw()
```

This boxplot graph confirms that **players who bring the most to their team offensively are more valued than others** (for this group, we note a median above 10 million euros). We also see that these Offensive players have a big variance in their Values distribution. This surely stems from the fact that **the most expensive players in the world (extreme positive outliers) often have a high offensive work rate**.


#### Plot of the Value vs Overall Grade

Another natural question that arise is about the correlation of the Value and the Overall grade a player gets in the FIFA video game. Here again, regarding the big gaps in values between Field Position, we faceted our graph according to Field Position.

```{r}
ggplot(data) +
  aes(group=Overall, y=`Value (in M€)`, fill=Overall) +
  geom_boxplot(colour="Black", size=0.3) +
  scale_fill_gradient2(low="Red", mid="Red", high="Blue", midpoint=73) +
  scale_x_continuous(limits=c(0.1,0.4)) +
  facet_grid(Field_Position~.) +
  theme(legend.position = "bottom",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

Obviously, the higher the grade, the higher the value. But what is interesting is that, for Defensers, this might not be as simple as this **when the overall grade gets the highest, the value seems to be steady**. For Goalkeepers too, the logic looks a bit more complex. Regarding Mitfielders and Attackers, the logic is respected and we have large variances in the Value.

Let's dig deeper into the causes of that variance.


#### Plot of the Value vs Age

In the context of short term career in professionnal football, we wanted to ask how Age was linked with the Value of Players. So we group our dataset by Age and computed a percentage ratio to visualize this distribtion. Comparing Age and Value in a two-axis graph with boxplot for each Age give the following graph:

```{r}
dfgraph <- data %>% 
  group_by(Age) %>% 
  summarize(Generation=n()) %>% 
  mutate(GenPerc = Generation/sum(Generation)) %>% 
  right_join(data, by="Age")

ggplot(dfgraph) +
  aes(x=Age, y=`Value (in M€)`, fill=GenPerc) +
  geom_boxplot(aes(group=Age), alpha=0.5, size=0.3) +
  scale_x_continuous(limits=c(15,42)) +
  scale_y_log10() +
  scale_fill_gradient("% of Tot. Population", low="yellow", high="red") +
  theme_bw()+
  theme(legend.position = "bottom",
        legend.title = element_text(face="bold"))
```

First of all, the Age distribution in our dataset goes from 16 to 41 with, as expected the highest percentage of total population between 21 and 26 years old. However, the biggest Values in Millions of euros are achieved between 27 and 30 years old, meaning that experience is at some point significant for evaluating players' Values. One can also notice the important number of outliers for Age between 19 and 23 years, whom can be identified as "young gifted players". And finally after 31 years old, the Age representation in the total population of professional players decreases, so does the Value. There is also more room for outliers at this Age, explaining the large variances from a year to another.

#### Plot of the Value vs Age and Overall Grade

Let's now put all together the two previous graphs with this last one placing Age on the x-axis and Overall Grade on the y-axis, we want to clarify how the experience influences the overall grade and skills of a player, and also how well this Overall Grade is represented through Value in million euros (here quantified through color opacity).

```{r}
plot_central = ggplot(data) + 
                  aes(x=Age, y=Overall, colour=log(`Value (in M€)`)) +
                  geom_point(stroke=F, alpha=0.7) +
                  geom_hline(yintercept = median(data$Overall), lty=2, alpha=0.5) +
                  geom_vline(xintercept = median(data$Age), lty=2, alpha=0.5) +
                  geom_smooth(method="loess", size=0.3, se=F, colour="black", show.legend = T) +
                  theme_bw() +
                  scale_color_continuous("Transfer Value", low="white", high="blue", labels=NULL) +
                  scale_x_continuous(limits=c(16, 40)) +
                  theme(legend.position="bottom", legend.title = element_text(size=10),
                        axis.title.x = element_text(face="bold"),
                        axis.title.y = element_text(face="bold"))

ggMarginal(plot_central, type="boxplot", col="darkblue")
```

Statistics shows that medians are 25 for Age and 66 for Overall. We find again out outliers in the Overall Grade distribution, explained by the highly known and talented players (that are also the highest in Transfer Value). The smoothing line shows something interesting, there is at some point a decrease in the Overall Grade (starting after 30 years old). This depreciation of the grade is also represented in the transfer Value, even if minimum Overall grade tends to highly increase as long as Age is increasing (46 for the 'below-20', 57 for the 'above-40').

***

# III-Regression models : Predict Value with inputs

As a reminder, our goal in this FIFA 19 case is to predict - i.e. to find the best machine that explain - the Value in Million euros of professional football players. 

Mathematically speaking, this variable to explain, the Y, is continuous. Hence, we are faced with a regression problem. As a starting statement, we propose to make a correlogram of the explaining variables, the Xs.

**Correlogram of the dataset:**

```{r}
datacor <- data %>% select(-Name,
                           -Nationality,
                           -Club,
                           -league_name,
                           -Field_Position)

datacor$`Preferred Foot` <- as.numeric(datacor$`Preferred Foot` == "Right")
datacor$`International Reputation` <- as.numeric(datacor$`International Reputation`)
datacor$`Weak Foot` <- as.numeric(datacor$`Weak Foot`)
datacor$`Skill Moves` <- as.numeric(datacor$`Skill Moves`)
datacor$`Offensive Work Rate` <- as.numeric(datacor$`Offensive Work Rate`)
datacor$`Defensive Work Rate` <- as.numeric(datacor$`Defensive Work Rate`)

str(datacor)
corr <- cor(datacor)

ggcorrplot(corr, method = "square", tl.cex=5, tl.srt=60)
```

To avoid several unrelevant variables and dummies in our analysis, we clear the dataset from some columns:
-ID
-Name
-Nationality
-Club
-num_club

```{r}
datareg <- data %>% select(-ID, -Name, -Nationality, -Club, -num_club)
```

For comparing the following machines, we will compute the **mean squared error** and each algorithm will be performed using a **20 times cross-validation**. 

```{r}
folds <- createFolds(1:nrow(datareg),k=20)
```

**Linear regression:**

First of all, we begin with a classical linear regression model, with big assumption but a good starting point for any prediction problem.

```{r}
pred_lm =  rep(NA,nrow(datareg))

for (i in 1:20){
  train <- datareg %>% slice(-folds[[i]])
  test <- datareg %>% slice(folds[[i]])
  linear_model <- lm(`Value (in M€)`~., data=train)
  pred_lm[folds[[i]]]=predict(linear_model,newdata=test)
}

mean((pred_lm-datareg$`Value (in M€)`)^2)
```


With a high MSE, and a large number of predictors (more than 50), we will now fit two penalized regression model, a ridge regression and a lasso regression

**Ridge:**

```{r}
X = model.matrix(`Value (in M€)`~.,data=datareg)[,-1]

my_ridge = cv.glmnet(X,datareg$`Value (in M€)`,alpha = 0,lambda=exp(seq(-8,4,length=100)))
plot(my_ridge)
best_lambda_ridge = my_ridge$lambda.min

pred_ridge_linear =  rep(NA, nrow(datareg))
for (i in 1:20){
  train.X = X[-folds[[i]],]
  train.Y = datareg$`Value (in M€)`[-folds[[i]]]
  test.X = X[folds[[i]],]
  ridge_linear = glmnet(train.X, train.Y, alpha = 0)
  pred_ridge_linear[folds[[i]]] = as.vector(predict(ridge_linear, newx = test.X, s = best_lambda_ridge))

}
```

**Lasso :**

```{r}
my_lasso = cv.glmnet(X,datareg$`Value (in M€)`,alpha = 1,lambda = exp(seq(-40,4,length = 200)))
plot(my_lasso)
best_lambda_lasso = my_lasso$lambda.min


pred_lasso_linear =  rep(NA,nrow(datareg))
for (i in 1:20){
  train.X = X[-folds[[i]],]
  train.Y = datareg$`Value (in M€)`[-folds[[i]]]
  test.X = X[folds[[i]],]
  lasso_linear = glmnet(train.X, train.Y, alpha = 1)
  pred_lasso_linear[folds[[i]]] = as.vector(predict(lasso_linear, newx = test.X, s = best_lambda_lasso))

}
```

Let's now compare our results in a table:

```{r}
prev_df = data.frame(pred_lm,pred_ridge_linear,pred_lasso_linear,observed=datareg$`Value (in M€)`)
prev_df %>% summarize_all(~mean((.-observed)^2)) %>% select(-observed)
```

The penalized regression are not really efficient machines in our case. The ridge parameters are even worse than the classical linear model. We will now make an analysis of the residuals for each machines to better understand what model we should use next. 

```{r}
df_residuals = data.frame(index = seq(1,nrow(prev_df)),
                        res_lm = prev_df$observed - prev_df$pred_lm,
                        res_ridge = prev_df$observed - prev_df$pred_ridge_linear,
                        res_lasso = prev_df$observed - prev_df$pred_lasso_linear)
```

#We take the index of top 10 highest residuals for each model

```{r}
high_res_lm = head(order(abs(df_residuals$res_lm),decreasing = T),10)
high_res_ridge = head(order(abs(df_residuals$res_ridge),decreasing = T),10)
high_res_lasso = head(order(abs(df_residuals$res_lasso),decreasing = T),10)
```

Which players are the outliers ? They are almost the same players for each model.

```{r}
residuals_players = data.frame(data[high_res_lm,'Name'],
                              data[high_res_ridge,'Name'],
                              data[high_res_lasso,'Name'])
colnames(residuals_players) = c('lm','ridge','lasso')
residuals_players

df_residuals2 = df_residuals %>% gather(key = model, value = residuals, - index)
```

We now visualise residual plots to see that we have large outliers but not too many. Let's also compute the MSE.

```{r}
ggplot(df_residuals2) +
  aes(x = index, y = residuals, color = model) +
  geom_point() +
  facet_grid(~model)


MSE = prev_df %>% slice(-(1:2000)) %>% summarize_all(~mean((.-observed)^2))
MSE
```

We could remove them from the sample to see that the MSE is better, but with deeper analysis we understand that the MAPE remains quite high with this change because we have lower values but still a high comparative error.


```{r}
MAPE = prev_df %>% slice(-(1:2000)) %>% summarize_all(~mean(abs((observed-.)/(observed))))
MAPE
```

We conclude that we won't remove these outliers from the sample, and that the linear model is not the most relevant one.

We will now move to non-parametric models to investigate on which set of model is the best fit for our FIFA 19 dataset.


**Non parametric model : k-nearest neighbors**

First, find the best k with *caret*:

```{r}
X = model.matrix(`Value (in M€)`~.,data=datareg)[,-1]
dataknn = data.frame(Y=datareg$`Value (in M€)`,X)

grid.k = data.frame(k=seq(1,5,by=1)) #We took a low number to reduce comuting time
ctrl = trainControl(method="cv",number=5)
select.k = train(Y~.,data =  dataknn, method="knn",trControl=ctrl,tuneGrid=grid.k)
best.k=select.k$bestTune

```

Then, we fit the knn model with the best k:

```{r}
pred_knn = rep(NA,nrow(dataknn))
for (i in 1:20) {
pred_knn[folds[[i]]]=as.numeric(as.vector(knn(train = dataknn[-folds[[i]],2:69] , test = dataknn[folds[[i]],2:69] ,cl = dataknn[-folds[[i]],1], k=best.k)))
}

```

Let's analyse our results : 

```{r}
prev_df_knn = data.frame(pred_knn, Y = dataknn$Y)

MSE_knn = prev_df_knn %>% summarize(MSE_knn=mean((pred_knn-Y)^2))
MSE_knn
```

We have definitely better results when using k-nearest neighbors algorithm.

**Non parametric model : Kernel**

As we need only 1 variable as input in this model, we have decided to do a PCA and keep only the first principal component.

PCA methodology:

```{r}
datakernel = data.frame(Y = data$`Value (in M€)`, X)
my.pca = princomp(datakernel[,-1])
summary(my.pca)

loadings = as.matrix(my.pca$loadings[,1])

datakernel_pca = data.frame(X = my.pca$scores[,1], Y = datakernel$Y)

plot(datakernel_pca[1:500,])

plot(datakernel_pca)
```

We take only the first principal component to do kernel regression (it represents almost 90% of the total variance so we are satisfied). We tried to visualise the coefficient of each variable, yet here it is difficult to interpret.

We take only scores for each player of the first PC. And for the first 500 players, it seems that there is a negative relation between PCA scores and the Value variable.

Nevertheless, for the whole data it seems that there is no relation between scores and market values but let's test it now.

Beforehand, let us find the best bandwidth and results:

```{r}
pred_kernel = rep(NA,nrow(datakernel_pca))
MSE_kern=rep(NA,100)

for (j in 1:200) {
    for (i in 1:20){
    kernel =  ksmooth(datakernel_pca[-folds[[i]],1],
                      datakernel_pca[-folds[[i]],2],
                      bandwidth = j/10, #we want to test bandwidths from 0.1 to 20
                      x.points = datakernel[folds[[i]],1])
    
    pred_kernel[folds[[i]]] = kernel$y 
    }
MSE_kern[j] = mean((pred_kernel-datakernel_pca$Y)^2,na.rm=T) 
}

min(MSE_kern) 
```

The lowest MSE is incredibly high : the kernel model with PCA is not relevant for our business problem.

**Trees and Random forests**

The last family of machine that we will try is Trees and random forests. For the first one, we will try to find the optimal number of splits using the 20 times cross-validation. However, for the second one, we assume that the parameter *mtry* is set to default value 17 (i.e. the number of dimension divided by 3, rounded). With more computing power, we would have try to get a better tuning parameter *mtry*.

```{r}
pred.tree = rep(NA,nrow(datareg))

cp_opt = rep (NA,20)
for (i in 1:20) {
train = datareg %>% slice(-folds[[i]])
test = datareg %>% slice(folds[[i]])
tree = rpart(`Value (in M€)`~., data=train)
cp_opt [i] = tree$cptable %>% as.data.frame() %>% filter(xerror==min(xerror)) %>% select(CP) %>% as.numeric()
besttree = prune(tree,cp=cp_opt[i])
pred.tree[folds[[i]]] = predict(besttree,newdata = test)
}

prev_df = (prev_df %>% mutate(pred.tree))[,c(1,2,3,5,4)]

prev_df %>% summarize_all(~mean((.-observed)^2))

print(cp_opt)

visTree(besttree)
```

It appears clearly here that with Trees we have a better MSE. One can notify that the optimal CP is the same for all the trees that we fitted, meaning that there is no much variation between tree used for each fold. One can visualize a example of Tree with visNetwork package. On the later, we are seeing that the Overall Grade and the Release clause are used as splitting variables.

Let's now find out if Random Forests can help us in getting a better MSE.

```{r}
x = as.data.frame(datareg[,-4]) ; y = as.vector(datareg[,4])
datareg2 = data.frame(x,y)
#We transform here the data into a data frame to keep a consistent format for the random forest machine.

m = round(length(x)/3)

pred_rf = rep(NA,nrow(datareg2))
for (i in 1:20){
  train = datareg2 %>% slice(-folds[[i]])
  test = datareg2 %>% slice(folds[[i]])
  rf = ranger(Value..in.M..~.,data=train,num.trees = 500,mtry=m)
  pred_rf[folds[[i]]] = predict(rf,data=test)$predictions
}

prev_df = (prev_df %>% mutate(pred_rf))[,c(1,2,3,4,6,5)]
prev_df %>% summarize_all(~mean((.-observed)^2))
```

## Conclusion: 

Comparing all MSE, we now know that **Random Forest is the best machine in predicting players' Value in million euros**. To keep increase performance and under assumption that we have more computation power, we would select a better tuning parameter using the caret package.
